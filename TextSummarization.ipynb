{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b74e5-75b3-43d7-8882-00a97b5f223d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551be2e9-959e-4460-9a93-468a8599cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parquet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "from src.features.functions_preprocessing import (\n",
    "    plot_text_length_distribution,\n",
    "    preprocess_articles,\n",
    "    preprocess_summaries,\n",
    ")\n",
    "from src.features.tokenization import parallel_tokenize\n",
    "from src.models.bert import BertSummary\n",
    "from src.models.rnn_encoder_decoder import Encoder, Decoder, Seq2Seq\n",
    "from src.models.transformer import Transformer\n",
    "from src.models.train_models import train_model\n",
    "from src.evaluation.model_evaluation import (\n",
    "    generate_summaries_seq2seq,\n",
    "    generate_summaries_transformer,\n",
    "    generate_summaries_bert,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4001c3-32fd-47cf-a1cf-7e092367a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307adead-5653-4210-ae8b-49feee29e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allowed_cpu_count():\n",
    "    # Returns the number of CPU cores available for this process.\n",
    "    try:\n",
    "        return len(os.sched_getaffinity(0))\n",
    "    except AttributeError:\n",
    "        return os.cpu_count() or 1\n",
    "\n",
    "\n",
    "cpu_count = get_allowed_cpu_count()\n",
    "print(cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a960e-9926-4d98-be1c-138870925f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_process = max(1, cpu_count // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41093c-9c96-4743-868e-7615d7602ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(n_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807d217-8730-4460-a6b4-f9b3af7dae71",
   "metadata": {},
   "source": [
    "# **Kaggle dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b3609-9ea8-436f-9ad5-2a8b6a74abe5",
   "metadata": {},
   "source": [
    "Download and extract the news summarization dataset from Kaggle, then load it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6cd36-482b-4138-b90e-b530d777364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d sbhatti/news-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae2436-3b97-4c81-9f05-fd46b54ca9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"news-summarization.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"news-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda220bf-ee5f-46a1-bc49-738d2433eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"news-summarization/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab595ed-3c4a-4432-a941-adbfa7223b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d61455-7eb4-46f6-b6bc-73e468d89870",
   "metadata": {},
   "source": [
    "We pick a random article from the dataset and display both its content and the corresponding summary to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031cbd9-5d20-4232-bc5b-013f17c33161",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = random.randint(1, len(news_data))\n",
    "\n",
    "print(news_data[\"Content\"][N])\n",
    "print()\n",
    "print(news_data[\"Summary\"][N])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9325b7-504c-4bb7-ab02-b2509e0c9ab9",
   "metadata": {},
   "source": [
    "We filter out very short and very long articles (outside the 10th and 90th percentiles) and then plot the length distribution of the remaining articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02e0f3-18ac-4587-a565-eb776f902183",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_article = news_data[\"Content\"].str.len()\n",
    "lengths_article.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705352d-0993-4db0-90c1-bb638cae4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[\n",
    "    (lengths_article >= lengths_article.quantile(0.10))\n",
    "    & (lengths_article <= lengths_article.quantile(0.90))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a65b8-3e3c-4d3e-bf74-dd6e0968f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_distribution(news_data, \"Content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa8e76-66d7-4a73-93bc-e559767978e8",
   "metadata": {},
   "source": [
    "We do the same for summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388fa01-72d3-4f19-be84-65cfb2f72616",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_summary = news_data[\"Summary\"].str.len()\n",
    "lengths_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9ae62-79d1-420e-aa5e-0b0f1b920add",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[\n",
    "    (lengths_summary >= lengths_summary.quantile(0.10))\n",
    "    & (lengths_summary <= lengths_summary.quantile(0.90))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8d5a5-dc67-4073-867e-8cc30911eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"Summary\"].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ef823-ef87-4cc4-8ec1-7f4418348be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_distribution(news_data, \"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebd3fb-536d-41de-a83e-e2a320b33623",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e99a3-85fa-4e80-98cf-e6033b3b7643",
   "metadata": {},
   "source": [
    "We preprocess the articles and summaries using parallel processing to clean and standardize the text data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d071a-a9d5-45b3-8dfb-62e2d20feddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data.loc[:, \"Content\"] = preprocess_articles(\n",
    "    news_data[\"Content\"].tolist(), n_process=n_process, batch_size=32\n",
    ")\n",
    "news_data.loc[:, \"Summary\"] = preprocess_summaries(\n",
    "    news_data[\"Summary\"].tolist(), n_process=n_process, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266194d-559a-4aee-8e81-929492060739",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.to_parquet(\"news_data_cleaned.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e907a-49bb-42f6-880b-0df87c38a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_parquet(\"news_data_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4606ea4-f7ab-4d51-b4ec-2f2c60e06446",
   "metadata": {},
   "source": [
    "# **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193944a9-2d8a-44f7-b7b5-f187507664a5",
   "metadata": {},
   "source": [
    "We shuffle the dataset, split it into training and testing sets with an 80-20 ratio, and print the sizes of both subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd31c4c-bbfa-4bb0-b3a7-4a3d28c87d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = news_data[:]\n",
    "data_copy = news_data.sample(frac=1, random_state=42)\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(data_copy))\n",
    "\n",
    "# Slice the dataset\n",
    "train_data = data_copy[:train_size]\n",
    "test_data = data_copy[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size:  {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24bd70-6ae9-48c4-b95d-739c4e7162d6",
   "metadata": {},
   "source": [
    "We tokenize the content of the articles & summaries in parallel using a BERT tokenizer, then save the tokenized result as a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b769e-51d2-437a-9220-a8c088c6bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_content = list(train_data[\"Content\"])\n",
    "    print(\"Tokenizing Content...\")\n",
    "    tokenized_articles = parallel_tokenize(\n",
    "        texts_content,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"tokenized_articles.shape =\", tokenized_articles.shape)\n",
    "    torch.save(tokenized_articles, \"tokenized_articles.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d607c-8677-4661-8894-5e060000d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_summary = list(train_data[\"Summary\"])\n",
    "    print(\"Tokenizing Summaries...\")\n",
    "    tokenized_summaries = parallel_tokenize(\n",
    "        texts_summary,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=129,\n",
    "    )\n",
    "    print(\"tokenized_summaries.shape =\", tokenized_summaries.shape)\n",
    "    torch.save(tokenized_summaries, \"tokenized_summaries.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992dcfe0-9bcc-4c25-bacc-f1da333bd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_content = list(test_data[\"Content\"])\n",
    "    print(\"Tokenizing Content...\")\n",
    "    tokenized_articles_test = parallel_tokenize(\n",
    "        texts_content,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"tokenized_articles.shape =\", tokenized_articles_test.shape)\n",
    "    torch.save(tokenized_articles_test, \"tokenized_articles_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59793caa-2cea-403e-b9e4-5760c866b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    tokenized_articles = torch.load(\"tokenized_articles.pt\")\n",
    "    tokenized_summaries = torch.load(\"tokenized_summaries.pt\")\n",
    "    tokenized_articles_test = torch.load(\"tokenized_articles_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1017c3e-aa9e-402f-aa0d-08560ba542f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids = tokenized_articles.long()\n",
    "summary_ids = tokenized_summaries.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e6163-0efa-4ca2-8819-9a5dcbd2115e",
   "metadata": {},
   "source": [
    "# **Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cba4d2-bbe6-480a-aae4-6ea8a31500fb",
   "metadata": {},
   "source": [
    "We create a dataset and dataloader, then initialize a Transformer model with BERT's vocabulary size, hidden size, 8 attention heads, and 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb018b-58a2-4b87-b6a1-0c156f3ab0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(tokenized_articles, tokenized_summaries)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=n_process, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd25b8e-33b9-4d2d-949a-9cacbd6485d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransformer = Transformer(\n",
    "    pad_idx=0,\n",
    "    voc_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    hidden_size=128,\n",
    "    n_head=8,\n",
    "    max_len=512,\n",
    "    dec_max_len=512,\n",
    "    ffn_hidden=128,\n",
    "    n_layers=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727fa72-18ba-4e75-84d9-81012141810b",
   "metadata": {},
   "source": [
    "We train the Transformer model for 25 epochs using the Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae5581-ad3c-4e79-ab68-accd2baf12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=modelTransformer,\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=25,\n",
    "    optimizer=torch.optim.Adam(modelTransformer.parameters(), lr=2e-4),\n",
    "    loss_fn=nn.CrossEntropyLoss(\n",
    "        ignore_index=BertTokenizer.from_pretrained(\"bert-base-uncased\").pad_token_id\n",
    "    ),\n",
    "    model_name=\"Transformer\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86c63-6434-48e0-9016-1b8b7a42792f",
   "metadata": {},
   "source": [
    "We initialize the Transformer model, load the pre-trained weights from a previous run (after 25 epochs), and set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074d384-280a-4e2c-9495-8d44f825e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransformer = Transformer(\n",
    "    pad_idx=0,\n",
    "    voc_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    hidden_size=128,\n",
    "    n_head=8,\n",
    "    max_len=512,\n",
    "    dec_max_len=128,\n",
    "    ffn_hidden=128,\n",
    "    n_layers=3,\n",
    ")\n",
    "modelTransformer.load_state_dict(\n",
    "    torch.load(\"model_weights/transformer_weights_25_epochs.pth\")\n",
    ")\n",
    "modelTransformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2404b2-f088-4e07-a8fc-5134f77e2c70",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e54d5f-ecf8-4793-9700-9eb597b7103c",
   "metadata": {},
   "source": [
    "We load the ROUGE evaluation metric, which is commonly used to assess the quality of generated text summaries by comparing them to reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86549c6a-cebb-4a60-adf3-c1dcc84eca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754ec6c-8cb3-42c9-9167-1ee514b15539",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_transformer = generate_summaries_transformer(\n",
    "    model=modelTransformer,\n",
    "    batch_size=32,\n",
    "    tokenized_input=tokenized_articles_test,\n",
    "    limit=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24df4c-b900-401c-a723-4817624996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[:, \"predictions_transformer\"] = predictions_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85f138-4573-4b1f-899b-161d475dd725",
   "metadata": {},
   "source": [
    "We compute ROUGE metrics by comparing the Transformer model's generated summaries to the reference summaries from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff35cc7-7fdb-4d73-95a8-af473da2b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = list(test_data[\"Summary\"])\n",
    "results = rouge.compute(\n",
    "    predictions=predictions_transformer, references=reference_summaries\n",
    ")\n",
    "print(\"ROUGE metrics:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
