{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b74e5-75b3-43d7-8882-00a97b5f223d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551be2e9-959e-4460-9a93-468a8599cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parquet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "from src.features.functions_preprocessing import (\n",
    "    plot_text_length_distribution,\n",
    "    preprocess_articles,\n",
    "    preprocess_summaries,\n",
    ")\n",
    "from src.features.tokenization import parallel_tokenize\n",
    "from src.models.bert import BertSummary\n",
    "from src.models.rnn_encoder_decoder import Encoder, Decoder, Seq2Seq\n",
    "from src.models.transformer import Transformer\n",
    "from src.models.train_models import train_model\n",
    "from src.evaluation.model_evaluation import (\n",
    "    generate_summaries_seq2seq,\n",
    "    generate_summaries_transformer,\n",
    "    generate_summaries_bert,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4001c3-32fd-47cf-a1cf-7e092367a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307adead-5653-4210-ae8b-49feee29e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allowed_cpu_count():\n",
    "    # Returns the number of CPU cores available for this process.\n",
    "    try:\n",
    "        return len(os.sched_getaffinity(0))\n",
    "    except AttributeError:\n",
    "        return os.cpu_count() or 1\n",
    "\n",
    "\n",
    "cpu_count = get_allowed_cpu_count()\n",
    "print(cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a960e-9926-4d98-be1c-138870925f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_process = max(1, cpu_count // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41093c-9c96-4743-868e-7615d7602ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(n_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807d217-8730-4460-a6b4-f9b3af7dae71",
   "metadata": {},
   "source": [
    "# **Kaggle dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b3609-9ea8-436f-9ad5-2a8b6a74abe5",
   "metadata": {},
   "source": [
    "Download and extract the news summarization dataset from Kaggle, then load it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6cd36-482b-4138-b90e-b530d777364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d sbhatti/news-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae2436-3b97-4c81-9f05-fd46b54ca9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"news-summarization.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"news-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda220bf-ee5f-46a1-bc49-738d2433eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"news-summarization/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab595ed-3c4a-4432-a941-adbfa7223b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d61455-7eb4-46f6-b6bc-73e468d89870",
   "metadata": {},
   "source": [
    "We pick a random article from the dataset and display both its content and the corresponding summary to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031cbd9-5d20-4232-bc5b-013f17c33161",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = random.randint(1, len(news_data))\n",
    "\n",
    "print(news_data[\"Content\"][N])\n",
    "print()\n",
    "print(news_data[\"Summary\"][N])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9325b7-504c-4bb7-ab02-b2509e0c9ab9",
   "metadata": {},
   "source": [
    "We filter out very short and very long articles (outside the 10th and 90th percentiles) and then plot the length distribution of the remaining articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02e0f3-18ac-4587-a565-eb776f902183",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_article = news_data[\"Content\"].str.len()\n",
    "lengths_article.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705352d-0993-4db0-90c1-bb638cae4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[\n",
    "    (lengths_article >= lengths_article.quantile(0.10))\n",
    "    & (lengths_article <= lengths_article.quantile(0.90))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a65b8-3e3c-4d3e-bf74-dd6e0968f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_distribution(news_data, \"Content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa8e76-66d7-4a73-93bc-e559767978e8",
   "metadata": {},
   "source": [
    "We do the same for summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388fa01-72d3-4f19-be84-65cfb2f72616",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_summary = news_data[\"Summary\"].str.len()\n",
    "lengths_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9ae62-79d1-420e-aa5e-0b0f1b920add",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[\n",
    "    (lengths_summary >= lengths_summary.quantile(0.10))\n",
    "    & (lengths_summary <= lengths_summary.quantile(0.90))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8d5a5-dc67-4073-867e-8cc30911eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"Summary\"].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ef823-ef87-4cc4-8ec1-7f4418348be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_distribution(news_data, \"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebd3fb-536d-41de-a83e-e2a320b33623",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e99a3-85fa-4e80-98cf-e6033b3b7643",
   "metadata": {},
   "source": [
    "We preprocess the articles and summaries using parallel processing to clean and standardize the text data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d071a-a9d5-45b3-8dfb-62e2d20feddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data.loc[:, \"Content\"] = preprocess_articles(\n",
    "    news_data[\"Content\"].tolist(), n_process=n_process, batch_size=32\n",
    ")\n",
    "news_data.loc[:, \"Summary\"] = preprocess_summaries(\n",
    "    news_data[\"Summary\"].tolist(), n_process=n_process, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266194d-559a-4aee-8e81-929492060739",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.to_parquet(\"news_data_cleaned.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e907a-49bb-42f6-880b-0df87c38a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_parquet(\"news_data_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4606ea4-f7ab-4d51-b4ec-2f2c60e06446",
   "metadata": {},
   "source": [
    "# **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193944a9-2d8a-44f7-b7b5-f187507664a5",
   "metadata": {},
   "source": [
    "We shuffle the dataset, split it into training and testing sets with an 80-20 ratio, and print the sizes of both subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd31c4c-bbfa-4bb0-b3a7-4a3d28c87d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = news_data[:]\n",
    "data_copy = news_data.sample(frac=1, random_state=42)\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(data_copy))\n",
    "\n",
    "# Slice the dataset\n",
    "train_data = data_copy[:train_size]\n",
    "test_data = data_copy[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size:  {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24bd70-6ae9-48c4-b95d-739c4e7162d6",
   "metadata": {},
   "source": [
    "We tokenize the content of the articles & summaries in parallel using a BERT tokenizer, then save the tokenized result as a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b769e-51d2-437a-9220-a8c088c6bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_content = list(train_data[\"Content\"])\n",
    "    print(\"Tokenizing Content...\")\n",
    "    tokenized_articles = parallel_tokenize(\n",
    "        texts_content,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"tokenized_articles.shape =\", tokenized_articles.shape)\n",
    "    torch.save(tokenized_articles, \"tokenized_articles.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d607c-8677-4661-8894-5e060000d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_summary = list(train_data[\"Summary\"])\n",
    "    print(\"Tokenizing Summaries...\")\n",
    "    tokenized_summaries = parallel_tokenize(\n",
    "        texts_summary,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=129,\n",
    "    )\n",
    "    print(\"tokenized_summaries.shape =\", tokenized_summaries.shape)\n",
    "    torch.save(tokenized_summaries, \"tokenized_summaries.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992dcfe0-9bcc-4c25-bacc-f1da333bd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_content = list(test_data[\"Content\"])\n",
    "    print(\"Tokenizing Content...\")\n",
    "    tokenized_articles_test = parallel_tokenize(\n",
    "        texts_content,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"tokenized_articles.shape =\", tokenized_articles_test.shape)\n",
    "    torch.save(tokenized_articles_test, \"tokenized_articles_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59793caa-2cea-403e-b9e4-5760c866b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    tokenized_articles = torch.load(\"tokenized_articles.pt\")\n",
    "    tokenized_summaries = torch.load(\"tokenized_summaries.pt\")\n",
    "    tokenized_summaries_bert = torch.load(\"tokenized_summaries_bert.pt\")\n",
    "    tokenized_articles_test = torch.load(\"tokenized_articles_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089836ea-921b-4a8d-9a84-21e37a9dddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_summaries_bert = torch.cat(\n",
    "    [torch.zeros(tokenized_summaries_bert.size(0), 1), tokenized_summaries_bert], dim=1\n",
    ").long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1017c3e-aa9e-402f-aa0d-08560ba542f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids = tokenized_articles.long()\n",
    "summary_ids = tokenized_summaries.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48ae10-ef79-46f3-9a79-8762cc67f344",
   "metadata": {},
   "source": [
    "# **Seq2Seq**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36d339-1d2b-4842-9a8c-2136665a9317",
   "metadata": {},
   "source": [
    "We create a dataset and dataloader for training, then initialize an encoder and decoder, and finally the Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376bb13-c135-459b-a93c-a25a5a2c2a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(tokenized_articles, tokenized_summaries)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=n_process, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08cbb3-2eca-4521-92f5-c8bea67de7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout_prob=0.1,\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "modelSeq2Seq = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19158d-cbe8-4798-babf-406e21f664fe",
   "metadata": {},
   "source": [
    "We train the Seq2Seq model for 30 epochs using the Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2f556-1591-44df-adfb-ea54d037cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=modelSeq2Seq,\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=30,\n",
    "    optimizer=torch.optim.Adam(modelSeq2Seq.parameters(), lr=2e-4),\n",
    "    loss_fn=nn.CrossEntropyLoss(\n",
    "        ignore_index=BertTokenizer.from_pretrained(\"bert-base-uncased\").pad_token_id\n",
    "    ),\n",
    "    model_name=\"Seq2Seq\",\n",
    "    device=device,\n",
    "    teacher_forcing_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd479c-de8e-4510-9afb-1fa5dc47d9e1",
   "metadata": {},
   "source": [
    "We initialize the encoder, decoder, and Seq2Seq model, load pre-trained weights from a previous run (after 25 epochs), and set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805db746-644a-486b-85c0-a338d78d05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout_prob=0.1,\n",
    ")\n",
    "decoder = Decoder(\n",
    "    vocab_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    embed_dim=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    dropout_prob=0.1,\n",
    ")\n",
    "\n",
    "modelSeq2Seq = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "modelSeq2Seq.load_state_dict(torch.load(\"model_weights/seq2seq_weights_25_epochs.pth\"))\n",
    "modelSeq2Seq.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e6163-0efa-4ca2-8819-9a5dcbd2115e",
   "metadata": {},
   "source": [
    "# **Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cba4d2-bbe6-480a-aae4-6ea8a31500fb",
   "metadata": {},
   "source": [
    "We create a dataset and dataloader, then initialize a Transformer model with BERT's vocabulary size, hidden size, 8 attention heads, and 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb018b-58a2-4b87-b6a1-0c156f3ab0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(tokenized_articles, tokenized_summaries)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=n_process, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd25b8e-33b9-4d2d-949a-9cacbd6485d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransformer = Transformer(\n",
    "    pad_idx=0,\n",
    "    voc_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    hidden_size=128,\n",
    "    n_head=8,\n",
    "    max_len=512,\n",
    "    dec_max_len=512,\n",
    "    ffn_hidden=128,\n",
    "    n_layers=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727fa72-18ba-4e75-84d9-81012141810b",
   "metadata": {},
   "source": [
    "We train the Transformer model for 25 epochs using the Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae5581-ad3c-4e79-ab68-accd2baf12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=modelTransformer,\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=25,\n",
    "    optimizer=torch.optim.Adam(modelTransformer.parameters(), lr=2e-4),\n",
    "    loss_fn=nn.CrossEntropyLoss(\n",
    "        ignore_index=BertTokenizer.from_pretrained(\"bert-base-uncased\").pad_token_id\n",
    "    ),\n",
    "    model_name=\"Transformer\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86c63-6434-48e0-9016-1b8b7a42792f",
   "metadata": {},
   "source": [
    "We initialize the Transformer model, load the pre-trained weights from a previous run (after 25 epochs), and set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074d384-280a-4e2c-9495-8d44f825e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransformer = Transformer(\n",
    "    pad_idx=0,\n",
    "    voc_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    hidden_size=128,\n",
    "    n_head=8,\n",
    "    max_len=512,\n",
    "    dec_max_len=128,\n",
    "    ffn_hidden=128,\n",
    "    n_layers=3,\n",
    ")\n",
    "modelTransformer.load_state_dict(\n",
    "    torch.load(\"model_weights/transformer_weights_25_epochs.pth\")\n",
    ")\n",
    "modelTransformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ebdd59-cc9d-40a4-a97c-94961d40df5f",
   "metadata": {},
   "source": [
    "# **BERT model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79e799-09d2-40ba-9e68-ec8db5a3cfcf",
   "metadata": {},
   "source": [
    "We create a dataset and dataloader with smaller batch size, then configure a BERT model with 12 hidden layers, 12 attention heads, and standard BERT parameters, and finally initialize the BertSummary model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea203f80-b14e-4a4c-939c-d5128b8aa7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "dataset = TensorDataset(tokenized_articles, tokenized_summaries_bert[:, 1:])\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=n_process, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a157c-77fb-45ff-b45c-0ee6f73d8050",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    vocab_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca7381-2813-4167-94fc-f6c810f47d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBert = BertSummary(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e5c2e-3aa5-45e2-88d3-c59b951a80e6",
   "metadata": {},
   "source": [
    "We train the BERT-based summarization model for 7 epochs using the Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f6ff0c-9d93-44ff-9a87-ff1525ce1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=modelBert,\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=7,\n",
    "    optimizer=torch.optim.Adam(modelBert.parameters(), lr=1e-5),\n",
    "    loss_fn=nn.CrossEntropyLoss(\n",
    "        ignore_index=BertTokenizer.from_pretrained(\"bert-base-uncased\").pad_token_id\n",
    "    ),\n",
    "    model_name=\"BERT\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70568-b880-429c-8246-046302afbe7b",
   "metadata": {},
   "source": [
    "We initialize the BERT summarization model, load pre-trained weights from a previous run (after 7 epochs) and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710bcc62-6e95-4c54-a99e-363d0bca518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBert = BertSummary(config)\n",
    "modelBert.load_state_dict(torch.load(\"model_weights/bert_weights_7_epochs.pth\"))\n",
    "modelBert.to(device)\n",
    "modelBert.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2404b2-f088-4e07-a8fc-5134f77e2c70",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e54d5f-ecf8-4793-9700-9eb597b7103c",
   "metadata": {},
   "source": [
    "We load the ROUGE evaluation metric, which is commonly used to assess the quality of generated text summaries by comparing them to reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86549c6a-cebb-4a60-adf3-c1dcc84eca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4133254-29eb-4c04-a7ae-73dfe3164fc6",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "\n",
    "We generate summaries using the Seq2Seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f331c-c776-437b-9d10-8f57fa1df4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_seq2seq = generate_summaries_seq2seq(\n",
    "    model=modelSeq2Seq,\n",
    "    batch_size=32,\n",
    "    tokenized_input=tokenized_articles_test,\n",
    "    limit=10,\n",
    ")\n",
    "\n",
    "with open(\"predictions_seq2seq.json\", \"w\") as f:\n",
    "    json.dump(predictions_seq2seq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651726a-9251-4ce3-83fa-71e5cc0cefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_seq2seq.json\", \"r\") as f:\n",
    "    predictions_seq2seq = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63aa87-c366-4fc3-87f4-597fae9fa8b5",
   "metadata": {},
   "source": [
    "We compute ROUGE metrics by comparing the Seq2Seq model's generated summaries to the reference summaries from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad67362-8ecb-4bfd-bd4f-acc251d23042",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = list(test_data[\"Summary\"][:320])\n",
    "results = rouge.compute(predictions=predictions_seq2seq, references=reference_summaries)\n",
    "print(\"ROUGE metrics:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f553d-16ed-4b0f-a70a-cee2ef2ed239",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "We generate summaries using the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754ec6c-8cb3-42c9-9167-1ee514b15539",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_transformer = generate_summaries_transformer(\n",
    "    model=modelTransformer,\n",
    "    batch_size=32,\n",
    "    tokenized_input=tokenized_articles_test,\n",
    "    limit=10,\n",
    ")\n",
    "\n",
    "with open(\"predictions_transformer.json\", \"w\") as f:\n",
    "    json.dump(predictions_transformer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24df4c-b900-401c-a723-4817624996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_transformer.json\", \"r\") as f:\n",
    "    predictions_transformer = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85f138-4573-4b1f-899b-161d475dd725",
   "metadata": {},
   "source": [
    "We compute ROUGE metrics by comparing the Transformer model's generated summaries to the reference summaries from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff35cc7-7fdb-4d73-95a8-af473da2b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = list(test_data[\"Summary\"][:320])\n",
    "results = rouge.compute(\n",
    "    predictions=predictions_transformer, references=reference_summaries\n",
    ")\n",
    "print(\"ROUGE metrics:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f0cc6-c85f-4e79-a0cf-cef50c8f3464",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5146f8c3-e097-4a5d-a916-07ba32ac2e7b",
   "metadata": {},
   "source": [
    "We generate summaries using the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba84033-a021-4a5c-9723-32f375d86269",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_bert = generate_summaries_bert(\n",
    "    model=modelBert,\n",
    "    batch_size=32,\n",
    "    tokenized_input=tokenized_articles_test,\n",
    "    limit=10,\n",
    ")\n",
    "\n",
    "with open(\"predictions_bert.json\", \"w\") as f:\n",
    "    json.dump(predictions_bert, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac77b82-57a3-477c-8678-fe79fe1bc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"predictions_bert.json\", \"r\") as f:\n",
    "    predictions_bert = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07156369-4eb4-4148-823f-d28fc9c489df",
   "metadata": {},
   "source": [
    "We compute ROUGE metrics by comparing the BERT model's generated summaries to the reference summaries from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea601b-fa71-4beb-b975-80b06b8c34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = list(test_data[\"Summary\"][:320])\n",
    "results = rouge.compute(predictions=predictions_bert, references=reference_summaries)\n",
    "print(\"ROUGE metrics:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
