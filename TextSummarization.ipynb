{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0b74e5-75b3-43d7-8882-00a97b5f223d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets (from -r requirements.txt (line 1))\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting kaggle (from -r requirements.txt (line 2))\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25done\n",
      "\u001b[?25hCollecting transformers (from -r requirements.txt (line 3))\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch (from -r requirements.txt (line 4))\n",
      "  Downloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting unidecode (from -r requirements.txt (line 5))\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting spacy (from -r requirements.txt (line 6))\n",
      "  Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting parquet (from -r requirements.txt (line 7))\n",
      "  Downloading parquet-1.3.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting evaluate (from -r requirements.txt (line 8))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score (from -r requirements.txt (line 9))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pre-commit (from -r requirements.txt (line 10))\n",
      "  Downloading pre_commit-4.1.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (2.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.2.3)\n",
      "Collecting ruff>=0.7.1 (from -r requirements.txt (line 13))\n",
      "  Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting pytest (from -r requirements.txt (line 15))\n",
      "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 1)) (8.32.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.12/site-packages (from ipywidgets->-r requirements.txt (line 1)) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets->-r requirements.txt (line 1))\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets->-r requirements.txt (line 1))\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (4.67.1)\n",
      "Collecting python-slugify (from kaggle->-r requirements.txt (line 2))\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.12/site-packages (from kaggle->-r requirements.txt (line 2)) (6.2.0)\n",
      "Collecting filelock (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 3))\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (2025.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (75.8.0)\n",
      "Collecting sympy==1.13.1 (from torch->-r requirements.txt (line 4))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->-r requirements.txt (line 4))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.12/site-packages (from spacy->-r requirements.txt (line 6)) (2.10.6)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting thriftpy2 (from parquet->-r requirements.txt (line 7))\n",
      "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate->-r requirements.txt (line 8))\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting dill (from evaluate->-r requirements.txt (line 8))\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from evaluate->-r requirements.txt (line 8))\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate->-r requirements.txt (line 8))\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting absl-py (from rouge_score->-r requirements.txt (line 9))\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (from rouge_score->-r requirements.txt (line 9)) (3.9.1)\n",
      "Collecting cfgv>=2.0.0 (from pre-commit->-r requirements.txt (line 10))\n",
      "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting identify>=1.0.0 (from pre-commit->-r requirements.txt (line 10))\n",
      "  Downloading identify-2.6.7-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting nodeenv>=0.11.1 (from pre-commit->-r requirements.txt (line 10))\n",
      "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting virtualenv>=20.10.0 (from pre-commit->-r requirements.txt (line 10))\n",
      "  Downloading virtualenv-20.29.2-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 12)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 12)) (2025.1)\n",
      "Collecting iniconfig (from pytest->-r requirements.txt (line 15))\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.12/site-packages (from pytest->-r requirements.txt (line 15)) (1.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (18.1.0)\n",
      "Collecting dill (from evaluate->-r requirements.txt (line 8))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate->-r requirements.txt (line 8))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 4))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (3.11.11)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.6.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6)) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->kaggle->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->kaggle->-r requirements.txt (line 2)) (3.10)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 6))\n",
      "  Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy->-r requirements.txt (line 6))\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6)) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 10))\n",
      "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.12/site-packages (from virtualenv>=20.10.0->pre-commit->-r requirements.txt (line 10)) (4.3.6)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.12/site-packages (from bleach->kaggle->-r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk->rouge_score->-r requirements.txt (line 9)) (1.4.2)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle->-r requirements.txt (line 2))\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting Cython>=3.0.10 (from thriftpy2->parquet->-r requirements.txt (line 7))\n",
      "  Using cached Cython-3.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting ply<4.0,>=3.4 (from thriftpy2->parquet->-r requirements.txt (line 7))\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 8)) (1.18.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.8.4)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.2.13)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6)) (1.17.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 1)) (0.2.3)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp312-cp312-manylinux1_x86_64.whl (766.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.6/766.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Downloading spacy-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading parquet-1.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading pre_commit-4.1.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading ruff-0.9.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading identify-2.6.7-py2.py3-none-any.whl (99 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading virtualenv-20.29.2-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached Cython-3.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: kaggle, rouge_score, thriftpy2\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?2done\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105839 sha256=d3e17e1f68032cfac683a0d0728a2b935da3990356f74c73d243334c600bb6c4\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/46/d2/26/84d0a1acdb9c6baccf7d28cf06962ec80529fe1ad938489983\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=bbccdcb554a1e37554a874e81a67766dacdd2eec955d538936df38d43f263f2c\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "  Building wheel for thriftpy2 (pyproject.toml) ..done\n",
      "\u001b[?25h  Created wheel for thriftpy2: filename=thriftpy2-0.5.2-cp312-cp312-linux_x86_64.whl size=844243 sha256=d700e65c1477127263492f9e6f20c40086b2653fffaf7e7444cf6ca9f5ec90d3\n",
      "  Stored in directory: /home/onyxia/.cache/pip/wheels/c9/ce/d5/93b65735501e7c6f4c8900c33a98cf9f2f1718490df69ad68c\n",
      "Successfully built kaggle rouge_score thriftpy2\n",
      "Installing collected packages: triton, text-unidecode, ply, nvidia-cusparselt-cu12, mpmath, distlib, cymem, xxhash, widgetsnbextension, wasabi, unidecode, sympy, spacy-loggers, spacy-legacy, smart-open, shellingham, safetensors, ruff, python-slugify, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nodeenv, murmurhash, mdurl, marisa-trie, jupyterlab-widgets, iniconfig, identify, fsspec, filelock, dill, Cython, cloudpathlib, cfgv, catalogue, blis, absl-py, virtualenv, thriftpy2, srsly, rouge_score, pytest, preshed, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, language-data, kaggle, huggingface-hub, tokenizers, rich, pre-commit, parquet, nvidia-cusolver-cu12, langcodes, ipywidgets, confection, typer, transformers, torch, thinc, datasets, weasel, evaluate, spacy\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2025.2.0 requires fsspec==2025.2.0.*, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Cython-3.0.11 absl-py-2.1.0 blis-1.2.0 catalogue-2.0.10 cfgv-3.4.0 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 datasets-3.2.0 dill-0.3.8 distlib-0.3.9 evaluate-0.4.3 filelock-3.17.0 fsspec-2024.9.0 huggingface-hub-0.28.1 identify-2.6.7 iniconfig-2.0.0 ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 kaggle-1.6.17 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 multiprocess-0.70.16 murmurhash-1.0.12 nodeenv-1.9.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 parquet-1.3.1 ply-3.11 pre-commit-4.1.0 preshed-3.0.9 pytest-8.3.4 python-slugify-8.0.4 rich-13.9.4 rouge_score-0.1.2 ruff-0.9.6 safetensors-0.5.2 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 sympy-1.13.1 text-unidecode-1.3 thinc-8.3.4 thriftpy2-0.5.2 tokenizers-0.21.0 torch-2.6.0 transformers-4.48.3 triton-3.2.0 typer-0.15.1 unidecode-1.3.8 virtualenv-20.29.2 wasabi-1.1.3 weasel-0.4.1 widgetsnbextension-4.0.13 xxhash-3.5.0\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551be2e9-959e-4460-9a93-468a8599cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parquet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "from src.features.functions_preprocessing import (\n",
    "    plot_text_length_distribution,\n",
    "    preprocess_articles,\n",
    "    preprocess_summaries,\n",
    ")\n",
    "from src.features.tokenization import parallel_tokenize\n",
    "from src.models.transformer import Transformer\n",
    "from src.models.train_models import train_model\n",
    "from src.evaluation.model_evaluation import (\n",
    "    generate_summaries_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4001c3-32fd-47cf-a1cf-7e092367a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307adead-5653-4210-ae8b-49feee29e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_allowed_cpu_count():\n",
    "    # Returns the number of CPU cores available for this process.\n",
    "    try:\n",
    "        return len(os.sched_getaffinity(0))\n",
    "    except AttributeError:\n",
    "        return os.cpu_count() or 1\n",
    "\n",
    "\n",
    "cpu_count = get_allowed_cpu_count()\n",
    "print(cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a960e-9926-4d98-be1c-138870925f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_process = max(1, cpu_count // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41093c-9c96-4743-868e-7615d7602ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(n_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807d217-8730-4460-a6b4-f9b3af7dae71",
   "metadata": {},
   "source": [
    "# **Kaggle dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b3609-9ea8-436f-9ad5-2a8b6a74abe5",
   "metadata": {},
   "source": [
    "Download and extract the news summarization dataset from Kaggle, then load it into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed6cd36-482b-4138-b90e-b530d777364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d sbhatti/news-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae2436-3b97-4c81-9f05-fd46b54ca9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"news-summarization.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"news-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda220bf-ee5f-46a1-bc49-738d2433eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(\"news-summarization/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab595ed-3c4a-4432-a941-adbfa7223b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d61455-7eb4-46f6-b6bc-73e468d89870",
   "metadata": {},
   "source": [
    "We pick a random article from the dataset and display both its content and the corresponding summary to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031cbd9-5d20-4232-bc5b-013f17c33161",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = random.randint(1, len(news_data))\n",
    "\n",
    "print(news_data[\"Content\"][N])\n",
    "print()\n",
    "print(news_data[\"Summary\"][N])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9325b7-504c-4bb7-ab02-b2509e0c9ab9",
   "metadata": {},
   "source": [
    "We filter out very short and very long articles (outside the 10th and 90th percentiles) and then plot the length distribution of the remaining articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad02e0f3-18ac-4587-a565-eb776f902183",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_article = news_data[\"Content\"].str.len()\n",
    "lengths_article.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a705352d-0993-4db0-90c1-bb638cae4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[\n",
    "    (lengths_article >= lengths_article.quantile(0.10))\n",
    "    & (lengths_article <= lengths_article.quantile(0.90))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a65b8-3e3c-4d3e-bf74-dd6e0968f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_distribution(news_data, \"Content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aa8e76-66d7-4a73-93bc-e559767978e8",
   "metadata": {},
   "source": [
    "We do the same for summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388fa01-72d3-4f19-be84-65cfb2f72616",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_summary = news_data[\"Summary\"].str.len()\n",
    "lengths_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9ae62-79d1-420e-aa5e-0b0f1b920add",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = news_data[\n",
    "    (lengths_summary >= lengths_summary.quantile(0.10))\n",
    "    & (lengths_summary <= lengths_summary.quantile(0.90))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8d5a5-dc67-4073-867e-8cc30911eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"Summary\"].str.len().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ef823-ef87-4cc4-8ec1-7f4418348be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_text_length_distribution(news_data, \"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebd3fb-536d-41de-a83e-e2a320b33623",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e99a3-85fa-4e80-98cf-e6033b3b7643",
   "metadata": {},
   "source": [
    "We preprocess the articles and summaries using parallel processing to clean and standardize the text data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d071a-a9d5-45b3-8dfb-62e2d20feddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_data.loc[:, \"Content\"] = preprocess_articles(\n",
    "    news_data[\"Content\"].tolist(), n_process=n_process, batch_size=32\n",
    ")\n",
    "news_data.loc[:, \"Summary\"] = preprocess_summaries(\n",
    "    news_data[\"Summary\"].tolist(), n_process=n_process, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266194d-559a-4aee-8e81-929492060739",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data.to_parquet(\"news_data_cleaned.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e907a-49bb-42f6-880b-0df87c38a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_parquet(\"news_data_cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4606ea4-f7ab-4d51-b4ec-2f2c60e06446",
   "metadata": {},
   "source": [
    "# **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193944a9-2d8a-44f7-b7b5-f187507664a5",
   "metadata": {},
   "source": [
    "We shuffle the dataset, split it into training and testing sets with an 80-20 ratio, and print the sizes of both subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd31c4c-bbfa-4bb0-b3a7-4a3d28c87d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = news_data[:]\n",
    "data_copy = news_data.sample(frac=1, random_state=42)\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(data_copy))\n",
    "\n",
    "# Slice the dataset\n",
    "train_data = data_copy[:train_size]\n",
    "test_data = data_copy[train_size:]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size:  {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24bd70-6ae9-48c4-b95d-739c4e7162d6",
   "metadata": {},
   "source": [
    "We tokenize the content of the articles & summaries in parallel using a BERT tokenizer, then save the tokenized result as a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b769e-51d2-437a-9220-a8c088c6bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_content = list(train_data[\"Content\"])\n",
    "    print(\"Tokenizing Content...\")\n",
    "    tokenized_articles = parallel_tokenize(\n",
    "        texts_content,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"tokenized_articles.shape =\", tokenized_articles.shape)\n",
    "    torch.save(tokenized_articles, \"tokenized_articles.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d607c-8677-4661-8894-5e060000d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_summary = list(train_data[\"Summary\"])\n",
    "    print(\"Tokenizing Summaries...\")\n",
    "    tokenized_summaries = parallel_tokenize(\n",
    "        texts_summary,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=129,\n",
    "    )\n",
    "    print(\"tokenized_summaries.shape =\", tokenized_summaries.shape)\n",
    "    torch.save(tokenized_summaries, \"tokenized_summaries.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992dcfe0-9bcc-4c25-bacc-f1da333bd7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    texts_content = list(test_data[\"Content\"])\n",
    "    print(\"Tokenizing Content...\")\n",
    "    tokenized_articles_test = parallel_tokenize(\n",
    "        texts_content,\n",
    "        tokenizer_name=\"bert-base-uncased\",\n",
    "        max_workers=n_process,\n",
    "        chunk_size=2000,\n",
    "        max_length=512,\n",
    "    )\n",
    "    print(\"tokenized_articles.shape =\", tokenized_articles_test.shape)\n",
    "    torch.save(tokenized_articles_test, \"tokenized_articles_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59793caa-2cea-403e-b9e4-5760c866b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    tokenized_articles = torch.load(\"tokenized_articles.pt\")\n",
    "    tokenized_summaries = torch.load(\"tokenized_summaries.pt\")\n",
    "    tokenized_articles_test = torch.load(\"tokenized_articles_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1017c3e-aa9e-402f-aa0d-08560ba542f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids = tokenized_articles.long()\n",
    "summary_ids = tokenized_summaries.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e6163-0efa-4ca2-8819-9a5dcbd2115e",
   "metadata": {},
   "source": [
    "# **Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cba4d2-bbe6-480a-aae4-6ea8a31500fb",
   "metadata": {},
   "source": [
    "We create a dataset and dataloader, then initialize a Transformer model with BERT's vocabulary size, hidden size, 8 attention heads, and 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb018b-58a2-4b87-b6a1-0c156f3ab0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = TensorDataset(tokenized_articles, tokenized_summaries)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, num_workers=n_process, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd25b8e-33b9-4d2d-949a-9cacbd6485d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransformer = Transformer(\n",
    "    pad_idx=0,\n",
    "    voc_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    hidden_size=128,\n",
    "    n_head=8,\n",
    "    max_len=512,\n",
    "    dec_max_len=512,\n",
    "    ffn_hidden=128,\n",
    "    n_layers=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727fa72-18ba-4e75-84d9-81012141810b",
   "metadata": {},
   "source": [
    "We train the Transformer model for 25 epochs using the Adam optimizer and cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae5581-ad3c-4e79-ab68-accd2baf12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model=modelTransformer,\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=25,\n",
    "    optimizer=torch.optim.Adam(modelTransformer.parameters(), lr=2e-4),\n",
    "    loss_fn=nn.CrossEntropyLoss(\n",
    "        ignore_index=BertTokenizer.from_pretrained(\"bert-base-uncased\").pad_token_id\n",
    "    ),\n",
    "    model_name=\"Transformer\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee86c63-6434-48e0-9016-1b8b7a42792f",
   "metadata": {},
   "source": [
    "We initialize the Transformer model, load the pre-trained weights from a previous run (after 25 epochs), and set the model to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074d384-280a-4e2c-9495-8d44f825e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTransformer = Transformer(\n",
    "    pad_idx=0,\n",
    "    voc_size=BertTokenizer.from_pretrained(\"bert-base-uncased\").vocab_size,\n",
    "    hidden_size=128,\n",
    "    n_head=8,\n",
    "    max_len=512,\n",
    "    dec_max_len=128,\n",
    "    ffn_hidden=128,\n",
    "    n_layers=3,\n",
    ")\n",
    "modelTransformer.load_state_dict(\n",
    "    torch.load(\"output/model_weights/transformer_weights_25_epochs.pth\")\n",
    ")\n",
    "modelTransformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2404b2-f088-4e07-a8fc-5134f77e2c70",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e54d5f-ecf8-4793-9700-9eb597b7103c",
   "metadata": {},
   "source": [
    "We load the ROUGE evaluation metric, which is commonly used to assess the quality of generated text summaries by comparing them to reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86549c6a-cebb-4a60-adf3-c1dcc84eca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754ec6c-8cb3-42c9-9167-1ee514b15539",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_transformer = generate_summaries_transformer(\n",
    "    model=modelTransformer,\n",
    "    batch_size=32,\n",
    "    tokenized_input=tokenized_articles_test,\n",
    "    limit=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24df4c-b900-401c-a723-4817624996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[:, \"predictions_transformer\"] = predictions_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85f138-4573-4b1f-899b-161d475dd725",
   "metadata": {},
   "source": [
    "We compute ROUGE metrics by comparing the Transformer model's generated summaries to the reference summaries from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff35cc7-7fdb-4d73-95a8-af473da2b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_summaries = list(test_data[\"Summary\"])\n",
    "results = rouge.compute(\n",
    "    predictions=predictions_transformer, references=reference_summaries\n",
    ")\n",
    "print(\"ROUGE metrics:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
