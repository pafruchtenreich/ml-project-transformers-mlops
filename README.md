# Transformer Exploration for News Summarization :newspaper:

This project explores Transformer-based models inspired by the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762). We use a Transformer model for **news article summarization** using the [Kaggle News Summarization dataset](https://www.kaggle.com/datasets/sbhatti/news-summarization):

---

## How to use the code

The code for this project is available in a modular format. All utility functions are organized into `.py` files, while a Jupyter notebook (`TextSummarization.ipynb`) orchestrates their execution.

Hereâ€™s how you can explore the repository:

1. **Run all cells** :hourglass_flowing_sand:: Fully reproduce the entire workflow!
   - Note: This requires significant computational resources and time for training.

2. **Use the Transformer weights** :mechanical_arm::
   - The folder `model_weights/` contains checkpoints for the Transformer model. You can use these to generate summaries without retraining.

3. **Examine generated summaries** :page_facing_up::
   - The folder `model_predictions/` holds sample outputs generated by the Transformer model.

---

## Results and Experiments

The model was trained on an NVIDIA A100 GPU with 40 GB of high-bandwidth memory :computer:

| Model                        | ROUGE-1 | ROUGE-2 | ROUGE-L | Train Time (ep)  | Params  |
|------------------------------|---------|---------|---------|------------------|---------|
| Transformer                  | 0.20    | 0.04    | 0.15    | ~ $1.6 \times 10^4$ s (25) | $1.25 \times 10^7$  |
| BERT                         | 0.13    | 0.01    | 0.10    | ~ $7.7 \times 10^4$ s (7) | $1.33 \times 10^8$  |


## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT

## References

Inspired by the original *Attention is all you need* paper [1] and implementation.

[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762)

```
@misc{vaswani2023attentionneed,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762},
}
```
