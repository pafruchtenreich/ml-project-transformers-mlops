# Transformer Exploration for News Summarization :newspaper:

This project explores Transformer-based models inspired by the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762). We compare three approaches for **news article summarization** using the [Kaggle News Summarization dataset](https://www.kaggle.com/datasets/sbhatti/news-summarization):

1. :repeat: A **traditional RNN encoder-decoder** model  
2. :zap: A **Transformer** model (as in *Attention Is All You Need*)  
3. :rocket: A **pre-trained BERT** model (fine-tuned for summarization)

---

## Results and Experiments

- **Metrics**: ROUGE-1, ROUGE-2, ROUGE-L, BLEU  
- **Training Cost**: Time per epoch, GPU usage, model parameters  
- **Performance**: Evaluate generated summaries against ground truth

| Model                        | ROUGE-1 | ROUGE-2 | ROUGE-L | Train Time (ep) | Params  |
|-----------------------------|---------|---------|---------|-----------------|---------|
| RNN Encoder-Decoder         | ...     | ...     | ...     | ...             | ...     |
| Transformer                 | ...     | ...     | ...     | ...             | ...     |
| BERT                        | ...     | ...     | ...     | ...             | ...     |
